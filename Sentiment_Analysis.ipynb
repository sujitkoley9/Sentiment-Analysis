{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment-Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sujitkoley9/Sentiment-Analysis/blob/master/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKt9nTjgFWI0",
        "colab_type": "text"
      },
      "source": [
        "<h1> <center>Sentiment Analysis </center> </h1>\n",
        "<b>Useful link: </b>\n",
        "\n",
        "\n",
        "1. https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/\n",
        "2. https://realpython.com/python-keras-text-classification/\n",
        "\n",
        "3. https://medium.com/jatana/report-on-text-classification-using-cnn-rnn-han-f0e887214d5f\n",
        "\n",
        "4. https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5Mbzt71IuXF",
        "colab_type": "text"
      },
      "source": [
        "# 1. **Importing packages**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKuhkGf7DkUo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import date, timedelta,datetime\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords,wordnet\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize,pos_tag\n",
        "\n",
        "import gensim\n",
        "from gensim.test.utils import get_tmpfile\n",
        "from gensim.summarization import keywords\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout,Flatten\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, GRU\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV2ftSn9K0lJ",
        "colab_type": "text"
      },
      "source": [
        "# 2 . **Reading required files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFCqPWO0LASj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movie_df = pd.read_csv('amazon_cells_labelled.txt', names=['review', 'sentiment'],sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBTvNGDOXH1Z",
        "colab_type": "text"
      },
      "source": [
        "# 1. **Logistic Regression**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qXqwQG0XPcQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "a7b62fef-8d07-4ae4-f58b-dfd6e12b7e5f"
      },
      "source": [
        "# Cleaning the text\n",
        "X = movie_df['review']\n",
        "y = movie_df['sentiment']\n",
        "\n",
        "\n",
        "\n",
        "# train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1000)\n",
        "\n",
        "#  fit the vectorizer\n",
        "vectorizer = TfidfVectorizer(max_df =1.0,min_df=1,analyzer='word')\n",
        "fit_obj = vectorizer.fit(X_train)\n",
        "\n",
        "# Transform the vectorizer\n",
        "X_train = fit_obj.transform(X_train)\n",
        "X_test = fit_obj.transform(X_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train, y_train)\n",
        "score = classifier.score(X_test, y_test)\n",
        "print(\"Accuracy:\", score)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf-ho4rDbEJe",
        "colab_type": "text"
      },
      "source": [
        "# 2. **Neural network without word embedding**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3flw2gzbLsr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "bb20a461-e29c-4038-d563-0a4545e92256"
      },
      "source": [
        "# Cleaning the text\n",
        "X = movie_df['review']\n",
        "y = movie_df['sentiment']\n",
        "\n",
        "\n",
        "\n",
        "# train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1000)\n",
        "\n",
        "#  fit the vectorizer\n",
        "vectorizer = TfidfVectorizer(max_df =1.0,min_df=1,analyzer='word')\n",
        "fit_obj = vectorizer.fit(X_train)\n",
        "\n",
        "# Transform the vectorizer\n",
        "X_train = fit_obj.transform(X_train)\n",
        "X_test = fit_obj.transform(X_test)\n",
        "\n",
        "# Neural Network \n",
        "classifier  = Sequential()\n",
        "classifier.add(Dense(10,input_shape=(X_train.shape[1],), activation = 'relu'))\n",
        "classifier.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "# model summary\n",
        "classifier.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 10)                15470     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 15,481\n",
            "Trainable params: 15,481\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXMAEjcddfQL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8c875bfa-be56-453e-e062-f5b0b9a150a9"
      },
      "source": [
        "# compile the model\n",
        "classifier.compile(loss='binary_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Train the model\n",
        "history = classifier.fit(X_train, y_train,\n",
        "                    epochs=100,\n",
        "                    verbose=1 ,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=X_train.shape[0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'scipy.sparse.csr.csr_matrix'>, <class 'NoneType'>\n",
            "Train on 750 samples, validate on 250 samples\n",
            "Epoch 1/100\n",
            "750/750 [==============================] - 0s 151us/sample - loss: 0.6926 - accuracy: 0.5120 - val_loss: 0.6917 - val_accuracy: 0.5600\n",
            "Epoch 2/100\n",
            "750/750 [==============================] - 0s 19us/sample - loss: 0.6910 - accuracy: 0.5533 - val_loss: 0.6911 - val_accuracy: 0.5640\n",
            "Epoch 3/100\n",
            "750/750 [==============================] - 0s 18us/sample - loss: 0.6894 - accuracy: 0.5893 - val_loss: 0.6905 - val_accuracy: 0.5760\n",
            "Epoch 4/100\n",
            "750/750 [==============================] - 0s 16us/sample - loss: 0.6877 - accuracy: 0.6293 - val_loss: 0.6898 - val_accuracy: 0.5960\n",
            "Epoch 5/100\n",
            "750/750 [==============================] - 0s 17us/sample - loss: 0.6861 - accuracy: 0.6600 - val_loss: 0.6891 - val_accuracy: 0.5960\n",
            "Epoch 6/100\n",
            "750/750 [==============================] - 0s 15us/sample - loss: 0.6845 - accuracy: 0.6973 - val_loss: 0.6885 - val_accuracy: 0.6120\n",
            "Epoch 7/100\n",
            "750/750 [==============================] - 0s 19us/sample - loss: 0.6828 - accuracy: 0.7373 - val_loss: 0.6878 - val_accuracy: 0.6280\n",
            "Epoch 8/100\n",
            "750/750 [==============================] - 0s 15us/sample - loss: 0.6811 - accuracy: 0.7613 - val_loss: 0.6871 - val_accuracy: 0.6520\n",
            "Epoch 9/100\n",
            "750/750 [==============================] - 0s 16us/sample - loss: 0.6794 - accuracy: 0.7880 - val_loss: 0.6863 - val_accuracy: 0.6640\n",
            "Epoch 10/100\n",
            "750/750 [==============================] - 0s 15us/sample - loss: 0.6776 - accuracy: 0.8120 - val_loss: 0.6856 - val_accuracy: 0.6720\n",
            "Epoch 11/100\n",
            "750/750 [==============================] - 0s 15us/sample - loss: 0.6758 - accuracy: 0.8347 - val_loss: 0.6848 - val_accuracy: 0.6760\n",
            "Epoch 12/100\n",
            "750/750 [==============================] - 0s 20us/sample - loss: 0.6739 - accuracy: 0.8453 - val_loss: 0.6839 - val_accuracy: 0.6800\n",
            "Epoch 13/100\n",
            "750/750 [==============================] - 0s 17us/sample - loss: 0.6720 - accuracy: 0.8587 - val_loss: 0.6831 - val_accuracy: 0.6760\n",
            "Epoch 14/100\n",
            "750/750 [==============================] - 0s 18us/sample - loss: 0.6700 - accuracy: 0.8747 - val_loss: 0.6822 - val_accuracy: 0.6720\n",
            "Epoch 15/100\n",
            "750/750 [==============================] - 0s 20us/sample - loss: 0.6680 - accuracy: 0.8867 - val_loss: 0.6813 - val_accuracy: 0.6880\n",
            "Epoch 16/100\n",
            "750/750 [==============================] - 0s 22us/sample - loss: 0.6660 - accuracy: 0.9027 - val_loss: 0.6803 - val_accuracy: 0.6880\n",
            "Epoch 17/100\n",
            "750/750 [==============================] - 0s 23us/sample - loss: 0.6639 - accuracy: 0.9067 - val_loss: 0.6793 - val_accuracy: 0.6960\n",
            "Epoch 18/100\n",
            "750/750 [==============================] - 0s 17us/sample - loss: 0.6617 - accuracy: 0.9133 - val_loss: 0.6783 - val_accuracy: 0.7080\n",
            "Epoch 19/100\n",
            "750/750 [==============================] - 0s 22us/sample - loss: 0.6595 - accuracy: 0.9227 - val_loss: 0.6773 - val_accuracy: 0.7160\n",
            "Epoch 20/100\n",
            "750/750 [==============================] - 0s 25us/sample - loss: 0.6573 - accuracy: 0.9240 - val_loss: 0.6763 - val_accuracy: 0.7200\n",
            "Epoch 21/100\n",
            "750/750 [==============================] - 0s 20us/sample - loss: 0.6550 - accuracy: 0.9267 - val_loss: 0.6753 - val_accuracy: 0.7240\n",
            "Epoch 22/100\n",
            "750/750 [==============================] - 0s 20us/sample - loss: 0.6527 - accuracy: 0.9360 - val_loss: 0.6742 - val_accuracy: 0.7240\n",
            "Epoch 23/100\n",
            "750/750 [==============================] - 0s 20us/sample - loss: 0.6503 - accuracy: 0.9400 - val_loss: 0.6732 - val_accuracy: 0.7280\n",
            "Epoch 24/100\n",
            "750/750 [==============================] - 0s 20us/sample - loss: 0.6479 - accuracy: 0.9427 - val_loss: 0.6721 - val_accuracy: 0.7280\n",
            "Epoch 25/100\n",
            "750/750 [==============================] - 0s 19us/sample - loss: 0.6455 - accuracy: 0.9480 - val_loss: 0.6711 - val_accuracy: 0.7280\n",
            "Epoch 26/100\n",
            "750/750 [==============================] - 0s 20us/sample - loss: 0.6430 - accuracy: 0.9533 - val_loss: 0.6700 - val_accuracy: 0.7240\n",
            "Epoch 27/100\n",
            "750/750 [==============================] - 0s 16us/sample - loss: 0.6405 - accuracy: 0.9560 - val_loss: 0.6689 - val_accuracy: 0.7200\n",
            "Epoch 28/100\n",
            "750/750 [==============================] - 0s 20us/sample - loss: 0.6379 - accuracy: 0.9573 - val_loss: 0.6678 - val_accuracy: 0.7200\n",
            "Epoch 29/100\n",
            "750/750 [==============================] - 0s 20us/sample - loss: 0.6353 - accuracy: 0.9587 - val_loss: 0.6667 - val_accuracy: 0.7160\n",
            "Epoch 30/100\n",
            "750/750 [==============================] - 0s 19us/sample - loss: 0.6327 - accuracy: 0.9613 - val_loss: 0.6656 - val_accuracy: 0.7200\n",
            "Epoch 31/100\n",
            "750/750 [==============================] - 0s 16us/sample - loss: 0.6301 - accuracy: 0.9640 - val_loss: 0.6644 - val_accuracy: 0.7120\n",
            "Epoch 32/100\n",
            "750/750 [==============================] - 0s 20us/sample - loss: 0.6274 - accuracy: 0.9627 - val_loss: 0.6633 - val_accuracy: 0.7120\n",
            "Epoch 33/100\n",
            "750/750 [==============================] - 0s 28us/sample - loss: 0.6247 - accuracy: 0.9627 - val_loss: 0.6622 - val_accuracy: 0.7200\n",
            "Epoch 34/100\n",
            "750/750 [==============================] - 0s 17us/sample - loss: 0.6220 - accuracy: 0.9653 - val_loss: 0.6610 - val_accuracy: 0.7200\n",
            "Epoch 35/100\n",
            "750/750 [==============================] - 0s 20us/sample - loss: 0.6192 - accuracy: 0.9653 - val_loss: 0.6599 - val_accuracy: 0.7200\n",
            "Epoch 36/100\n",
            "750/750 [==============================] - 0s 19us/sample - loss: 0.6164 - accuracy: 0.9693 - val_loss: 0.6587 - val_accuracy: 0.7200\n",
            "Epoch 37/100\n",
            "750/750 [==============================] - 0s 21us/sample - loss: 0.6136 - accuracy: 0.9707 - val_loss: 0.6575 - val_accuracy: 0.7200\n",
            "Epoch 38/100\n",
            "750/750 [==============================] - 0s 22us/sample - loss: 0.6108 - accuracy: 0.9707 - val_loss: 0.6563 - val_accuracy: 0.7120\n",
            "Epoch 39/100\n",
            "750/750 [==============================] - 0s 19us/sample - loss: 0.6080 - accuracy: 0.9720 - val_loss: 0.6551 - val_accuracy: 0.7160\n",
            "Epoch 40/100\n",
            "750/750 [==============================] - 0s 19us/sample - loss: 0.6052 - accuracy: 0.9733 - val_loss: 0.6539 - val_accuracy: 0.7160\n",
            "Epoch 41/100\n",
            "750/750 [==============================] - 0s 20us/sample - loss: 0.6023 - accuracy: 0.9733 - val_loss: 0.6527 - val_accuracy: 0.7160\n",
            "Epoch 42/100\n",
            "750/750 [==============================] - 0s 16us/sample - loss: 0.5994 - accuracy: 0.9733 - val_loss: 0.6515 - val_accuracy: 0.7200\n",
            "Epoch 43/100\n",
            "750/750 [==============================] - 0s 16us/sample - loss: 0.5966 - accuracy: 0.9733 - val_loss: 0.6503 - val_accuracy: 0.7240\n",
            "Epoch 44/100\n",
            "750/750 [==============================] - 0s 15us/sample - loss: 0.5937 - accuracy: 0.9747 - val_loss: 0.6491 - val_accuracy: 0.7280\n",
            "Epoch 45/100\n",
            "750/750 [==============================] - 0s 21us/sample - loss: 0.5908 - accuracy: 0.9760 - val_loss: 0.6479 - val_accuracy: 0.7360\n",
            "Epoch 46/100\n",
            "750/750 [==============================] - 0s 20us/sample - loss: 0.5879 - accuracy: 0.9760 - val_loss: 0.6467 - val_accuracy: 0.7360\n",
            "Epoch 47/100\n",
            "750/750 [==============================] - 0s 16us/sample - loss: 0.5850 - accuracy: 0.9760 - val_loss: 0.6455 - val_accuracy: 0.7320\n",
            "Epoch 48/100\n",
            "750/750 [==============================] - 0s 19us/sample - loss: 0.5821 - accuracy: 0.9760 - val_loss: 0.6443 - val_accuracy: 0.7320\n",
            "Epoch 49/100\n",
            "750/750 [==============================] - 0s 21us/sample - loss: 0.5791 - accuracy: 0.9773 - val_loss: 0.6431 - val_accuracy: 0.7360\n",
            "Epoch 50/100\n",
            "750/750 [==============================] - 0s 19us/sample - loss: 0.5762 - accuracy: 0.9760 - val_loss: 0.6419 - val_accuracy: 0.7320\n",
            "Epoch 51/100\n",
            "750/750 [==============================] - 0s 24us/sample - loss: 0.5733 - accuracy: 0.9747 - val_loss: 0.6407 - val_accuracy: 0.7320\n",
            "Epoch 52/100\n",
            "750/750 [==============================] - 0s 20us/sample - loss: 0.5704 - accuracy: 0.9760 - val_loss: 0.6395 - val_accuracy: 0.7320\n",
            "Epoch 53/100\n",
            "750/750 [==============================] - 0s 23us/sample - loss: 0.5674 - accuracy: 0.9773 - val_loss: 0.6383 - val_accuracy: 0.7320\n",
            "Epoch 54/100\n",
            "750/750 [==============================] - 0s 21us/sample - loss: 0.5645 - accuracy: 0.9773 - val_loss: 0.6371 - val_accuracy: 0.7320\n",
            "Epoch 55/100\n",
            "750/750 [==============================] - 0s 24us/sample - loss: 0.5615 - accuracy: 0.9773 - val_loss: 0.6359 - val_accuracy: 0.7280\n",
            "Epoch 56/100\n",
            "750/750 [==============================] - 0s 18us/sample - loss: 0.5586 - accuracy: 0.9773 - val_loss: 0.6347 - val_accuracy: 0.7280\n",
            "Epoch 57/100\n",
            "750/750 [==============================] - 0s 18us/sample - loss: 0.5556 - accuracy: 0.9773 - val_loss: 0.6335 - val_accuracy: 0.7280\n",
            "Epoch 58/100\n",
            "750/750 [==============================] - 0s 22us/sample - loss: 0.5527 - accuracy: 0.9773 - val_loss: 0.6323 - val_accuracy: 0.7320\n",
            "Epoch 59/100\n",
            "750/750 [==============================] - 0s 16us/sample - loss: 0.5497 - accuracy: 0.9773 - val_loss: 0.6311 - val_accuracy: 0.7360\n",
            "Epoch 60/100\n",
            "750/750 [==============================] - 0s 20us/sample - loss: 0.5468 - accuracy: 0.9773 - val_loss: 0.6299 - val_accuracy: 0.7360\n",
            "Epoch 61/100\n",
            "750/750 [==============================] - 0s 24us/sample - loss: 0.5438 - accuracy: 0.9773 - val_loss: 0.6287 - val_accuracy: 0.7360\n",
            "Epoch 62/100\n",
            "750/750 [==============================] - 0s 23us/sample - loss: 0.5408 - accuracy: 0.9787 - val_loss: 0.6275 - val_accuracy: 0.7360\n",
            "Epoch 63/100\n",
            "750/750 [==============================] - 0s 23us/sample - loss: 0.5379 - accuracy: 0.9787 - val_loss: 0.6264 - val_accuracy: 0.7360\n",
            "Epoch 64/100\n",
            "750/750 [==============================] - 0s 19us/sample - loss: 0.5349 - accuracy: 0.9800 - val_loss: 0.6252 - val_accuracy: 0.7360\n",
            "Epoch 65/100\n",
            "750/750 [==============================] - 0s 23us/sample - loss: 0.5320 - accuracy: 0.9800 - val_loss: 0.6240 - val_accuracy: 0.7360\n",
            "Epoch 66/100\n",
            "750/750 [==============================] - 0s 18us/sample - loss: 0.5290 - accuracy: 0.9800 - val_loss: 0.6228 - val_accuracy: 0.7360\n",
            "Epoch 67/100\n",
            "750/750 [==============================] - 0s 22us/sample - loss: 0.5261 - accuracy: 0.9800 - val_loss: 0.6216 - val_accuracy: 0.7360\n",
            "Epoch 68/100\n",
            "750/750 [==============================] - 0s 24us/sample - loss: 0.5231 - accuracy: 0.9813 - val_loss: 0.6205 - val_accuracy: 0.7360\n",
            "Epoch 69/100\n",
            "750/750 [==============================] - 0s 17us/sample - loss: 0.5202 - accuracy: 0.9813 - val_loss: 0.6193 - val_accuracy: 0.7360\n",
            "Epoch 70/100\n",
            "750/750 [==============================] - 0s 23us/sample - loss: 0.5173 - accuracy: 0.9813 - val_loss: 0.6181 - val_accuracy: 0.7360\n",
            "Epoch 71/100\n",
            "750/750 [==============================] - 0s 18us/sample - loss: 0.5143 - accuracy: 0.9813 - val_loss: 0.6170 - val_accuracy: 0.7320\n",
            "Epoch 72/100\n",
            "750/750 [==============================] - 0s 15us/sample - loss: 0.5114 - accuracy: 0.9813 - val_loss: 0.6158 - val_accuracy: 0.7320\n",
            "Epoch 73/100\n",
            "750/750 [==============================] - 0s 23us/sample - loss: 0.5085 - accuracy: 0.9813 - val_loss: 0.6147 - val_accuracy: 0.7320\n",
            "Epoch 74/100\n",
            "750/750 [==============================] - 0s 18us/sample - loss: 0.5055 - accuracy: 0.9813 - val_loss: 0.6135 - val_accuracy: 0.7320\n",
            "Epoch 75/100\n",
            "750/750 [==============================] - 0s 18us/sample - loss: 0.5026 - accuracy: 0.9813 - val_loss: 0.6124 - val_accuracy: 0.7320\n",
            "Epoch 76/100\n",
            "750/750 [==============================] - 0s 18us/sample - loss: 0.4997 - accuracy: 0.9813 - val_loss: 0.6113 - val_accuracy: 0.7320\n",
            "Epoch 77/100\n",
            "750/750 [==============================] - 0s 19us/sample - loss: 0.4968 - accuracy: 0.9813 - val_loss: 0.6101 - val_accuracy: 0.7320\n",
            "Epoch 78/100\n",
            "750/750 [==============================] - 0s 20us/sample - loss: 0.4939 - accuracy: 0.9813 - val_loss: 0.6090 - val_accuracy: 0.7320\n",
            "Epoch 79/100\n",
            "750/750 [==============================] - 0s 16us/sample - loss: 0.4910 - accuracy: 0.9813 - val_loss: 0.6079 - val_accuracy: 0.7320\n",
            "Epoch 80/100\n",
            "750/750 [==============================] - 0s 20us/sample - loss: 0.4881 - accuracy: 0.9813 - val_loss: 0.6068 - val_accuracy: 0.7320\n",
            "Epoch 81/100\n",
            "750/750 [==============================] - 0s 19us/sample - loss: 0.4852 - accuracy: 0.9813 - val_loss: 0.6057 - val_accuracy: 0.7320\n",
            "Epoch 82/100\n",
            "750/750 [==============================] - 0s 19us/sample - loss: 0.4823 - accuracy: 0.9813 - val_loss: 0.6045 - val_accuracy: 0.7320\n",
            "Epoch 83/100\n",
            "750/750 [==============================] - 0s 18us/sample - loss: 0.4794 - accuracy: 0.9813 - val_loss: 0.6034 - val_accuracy: 0.7320\n",
            "Epoch 84/100\n",
            "750/750 [==============================] - 0s 18us/sample - loss: 0.4765 - accuracy: 0.9813 - val_loss: 0.6023 - val_accuracy: 0.7320\n",
            "Epoch 85/100\n",
            "750/750 [==============================] - 0s 21us/sample - loss: 0.4736 - accuracy: 0.9813 - val_loss: 0.6012 - val_accuracy: 0.7320\n",
            "Epoch 86/100\n",
            "750/750 [==============================] - 0s 20us/sample - loss: 0.4708 - accuracy: 0.9827 - val_loss: 0.6001 - val_accuracy: 0.7320\n",
            "Epoch 87/100\n",
            "750/750 [==============================] - 0s 22us/sample - loss: 0.4679 - accuracy: 0.9840 - val_loss: 0.5991 - val_accuracy: 0.7320\n",
            "Epoch 88/100\n",
            "750/750 [==============================] - 0s 18us/sample - loss: 0.4651 - accuracy: 0.9840 - val_loss: 0.5980 - val_accuracy: 0.7360\n",
            "Epoch 89/100\n",
            "750/750 [==============================] - 0s 16us/sample - loss: 0.4622 - accuracy: 0.9840 - val_loss: 0.5969 - val_accuracy: 0.7400\n",
            "Epoch 90/100\n",
            "750/750 [==============================] - 0s 18us/sample - loss: 0.4594 - accuracy: 0.9840 - val_loss: 0.5958 - val_accuracy: 0.7400\n",
            "Epoch 91/100\n",
            "750/750 [==============================] - 0s 22us/sample - loss: 0.4566 - accuracy: 0.9840 - val_loss: 0.5948 - val_accuracy: 0.7400\n",
            "Epoch 92/100\n",
            "750/750 [==============================] - 0s 22us/sample - loss: 0.4538 - accuracy: 0.9840 - val_loss: 0.5937 - val_accuracy: 0.7400\n",
            "Epoch 93/100\n",
            "750/750 [==============================] - 0s 23us/sample - loss: 0.4510 - accuracy: 0.9840 - val_loss: 0.5927 - val_accuracy: 0.7400\n",
            "Epoch 94/100\n",
            "750/750 [==============================] - 0s 17us/sample - loss: 0.4482 - accuracy: 0.9840 - val_loss: 0.5916 - val_accuracy: 0.7360\n",
            "Epoch 95/100\n",
            "750/750 [==============================] - 0s 22us/sample - loss: 0.4454 - accuracy: 0.9840 - val_loss: 0.5906 - val_accuracy: 0.7360\n",
            "Epoch 96/100\n",
            "750/750 [==============================] - 0s 19us/sample - loss: 0.4426 - accuracy: 0.9840 - val_loss: 0.5895 - val_accuracy: 0.7360\n",
            "Epoch 97/100\n",
            "750/750 [==============================] - 0s 19us/sample - loss: 0.4398 - accuracy: 0.9840 - val_loss: 0.5885 - val_accuracy: 0.7360\n",
            "Epoch 98/100\n",
            "750/750 [==============================] - 0s 24us/sample - loss: 0.4370 - accuracy: 0.9840 - val_loss: 0.5875 - val_accuracy: 0.7360\n",
            "Epoch 99/100\n",
            "750/750 [==============================] - 0s 20us/sample - loss: 0.4343 - accuracy: 0.9840 - val_loss: 0.5865 - val_accuracy: 0.7360\n",
            "Epoch 100/100\n",
            "750/750 [==============================] - 0s 16us/sample - loss: 0.4315 - accuracy: 0.9840 - val_loss: 0.5855 - val_accuracy: 0.7360\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YYSkKcRgEul",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "52bbf8ac-6ba9-4009-dfc2-490aabd17797"
      },
      "source": [
        "loss, accuracy = classifier.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'scipy.sparse.csr.csr_matrix'>, <class 'NoneType'>\n",
            "Accuracy: 0.736\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2L-d2CAjFcv",
        "colab_type": "text"
      },
      "source": [
        "# 3. **Neural network with word embedding**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKEyMwnjgoKK",
        "colab_type": "text"
      },
      "source": [
        "## 3.1 **LSTM**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqLdcnGajM0L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cleaning the text\n",
        "X = movie_df['review']\n",
        "y = movie_df['sentiment']\n",
        "\n",
        "# train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1000)\n",
        "\n",
        "# fit method\n",
        "tokenizer_obj = Tokenizer()\n",
        "tokenizer_obj.fit_on_texts(X_train) \n",
        "\n",
        "# pad sequences\n",
        "max_length = max([len(s.split()) for s in X_train])\n",
        "\n",
        "# define vocabulary size\n",
        "vocab_size = len(tokenizer_obj.word_index) + 1\n",
        "\n",
        "\n",
        "# Transform\n",
        "X_train_tokens =  tokenizer_obj.texts_to_sequences(X_train)\n",
        "X_test_tokens = tokenizer_obj.texts_to_sequences(X_test)\n",
        "\n",
        "# Padding\n",
        "X_train_pad = pad_sequences(X_train_tokens, maxlen=max_length, padding='post')\n",
        "X_test_pad = pad_sequences(X_test_tokens, maxlen=max_length, padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyd_H6a_bae9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "outputId": "df89c14c-c4b5-425f-db38-39951fac1201"
      },
      "source": [
        "# LSTM\n",
        "embedding_dim =100\n",
        "classifier  = Sequential()\n",
        "classifier.add(Embedding(input_dim=vocab_size, \n",
        "                         output_dim=embedding_dim, \n",
        "                         input_length=max_length))\n",
        "\n",
        "classifier.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "\n",
        "classifier.add(Dense(10, activation = 'relu'))\n",
        "\n",
        "classifier.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "# model summary\n",
        "classifier.summary()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      (None, 30, 100)           157400    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 10)                1010      \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 238,821\n",
            "Trainable params: 238,821\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "op2hs9pvcvjv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bd016134-bafb-48d0-e8bd-c4268372694d"
      },
      "source": [
        "# compile the model\n",
        "classifier.compile(loss='binary_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])\n",
        "# Train the model\n",
        "history = classifier.fit(X_train_pad, np.array(y_train),\n",
        "                    epochs=100,\n",
        "                    verbose=1 ,\n",
        "                    validation_data=(X_test_pad, np.array(y_test)),\n",
        "                    batch_size=X_test_pad.shape[0])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 750 samples, validate on 250 samples\n",
            "Epoch 1/100\n",
            "750/750 [==============================] - 2s 3ms/sample - loss: 0.6929 - accuracy: 0.5107 - val_loss: 0.6931 - val_accuracy: 0.5040\n",
            "Epoch 2/100\n",
            "750/750 [==============================] - 0s 614us/sample - loss: 0.6934 - accuracy: 0.4933 - val_loss: 0.6931 - val_accuracy: 0.5040\n",
            "Epoch 3/100\n",
            "750/750 [==============================] - 0s 654us/sample - loss: 0.6937 - accuracy: 0.4827 - val_loss: 0.6930 - val_accuracy: 0.5040\n",
            "Epoch 4/100\n",
            "750/750 [==============================] - 0s 626us/sample - loss: 0.6928 - accuracy: 0.5133 - val_loss: 0.6932 - val_accuracy: 0.4960\n",
            "Epoch 5/100\n",
            "750/750 [==============================] - 0s 649us/sample - loss: 0.6923 - accuracy: 0.5347 - val_loss: 0.6931 - val_accuracy: 0.4960\n",
            "Epoch 6/100\n",
            "750/750 [==============================] - 0s 636us/sample - loss: 0.6920 - accuracy: 0.5507 - val_loss: 0.6929 - val_accuracy: 0.5120\n",
            "Epoch 7/100\n",
            "750/750 [==============================] - 0s 632us/sample - loss: 0.6916 - accuracy: 0.5653 - val_loss: 0.6926 - val_accuracy: 0.5240\n",
            "Epoch 8/100\n",
            "750/750 [==============================] - 0s 624us/sample - loss: 0.6893 - accuracy: 0.5867 - val_loss: 0.6914 - val_accuracy: 0.5440\n",
            "Epoch 9/100\n",
            "750/750 [==============================] - 0s 620us/sample - loss: 0.6851 - accuracy: 0.6120 - val_loss: 0.6868 - val_accuracy: 0.5480\n",
            "Epoch 10/100\n",
            "750/750 [==============================] - 0s 623us/sample - loss: 0.6672 - accuracy: 0.6800 - val_loss: 0.6624 - val_accuracy: 0.6640\n",
            "Epoch 11/100\n",
            "750/750 [==============================] - 0s 596us/sample - loss: 0.5962 - accuracy: 0.7787 - val_loss: 0.5875 - val_accuracy: 0.7120\n",
            "Epoch 12/100\n",
            "750/750 [==============================] - 0s 651us/sample - loss: 0.4366 - accuracy: 0.8360 - val_loss: 0.5213 - val_accuracy: 0.7720\n",
            "Epoch 13/100\n",
            "750/750 [==============================] - 1s 668us/sample - loss: 0.2995 - accuracy: 0.9013 - val_loss: 0.6146 - val_accuracy: 0.7680\n",
            "Epoch 14/100\n",
            "750/750 [==============================] - 0s 661us/sample - loss: 0.2394 - accuracy: 0.9267 - val_loss: 0.6938 - val_accuracy: 0.7680\n",
            "Epoch 15/100\n",
            "750/750 [==============================] - 0s 626us/sample - loss: 0.1404 - accuracy: 0.9640 - val_loss: 0.7706 - val_accuracy: 0.7760\n",
            "Epoch 16/100\n",
            "750/750 [==============================] - 0s 639us/sample - loss: 0.1275 - accuracy: 0.9667 - val_loss: 0.8078 - val_accuracy: 0.7520\n",
            "Epoch 17/100\n",
            "750/750 [==============================] - 0s 666us/sample - loss: 0.0986 - accuracy: 0.9733 - val_loss: 0.7306 - val_accuracy: 0.7880\n",
            "Epoch 18/100\n",
            "750/750 [==============================] - 0s 629us/sample - loss: 0.0799 - accuracy: 0.9773 - val_loss: 0.7038 - val_accuracy: 0.8000\n",
            "Epoch 19/100\n",
            "750/750 [==============================] - 0s 637us/sample - loss: 0.0459 - accuracy: 0.9920 - val_loss: 0.8603 - val_accuracy: 0.7720\n",
            "Epoch 20/100\n",
            "750/750 [==============================] - 0s 647us/sample - loss: 0.0353 - accuracy: 0.9933 - val_loss: 0.8997 - val_accuracy: 0.7880\n",
            "Epoch 21/100\n",
            "750/750 [==============================] - 0s 641us/sample - loss: 0.0399 - accuracy: 0.9920 - val_loss: 1.0161 - val_accuracy: 0.7880\n",
            "Epoch 22/100\n",
            "750/750 [==============================] - 0s 639us/sample - loss: 0.0557 - accuracy: 0.9880 - val_loss: 1.1901 - val_accuracy: 0.7840\n",
            "Epoch 23/100\n",
            "750/750 [==============================] - 0s 645us/sample - loss: 0.0464 - accuracy: 0.9907 - val_loss: 1.3540 - val_accuracy: 0.7640\n",
            "Epoch 24/100\n",
            "750/750 [==============================] - 0s 626us/sample - loss: 0.0377 - accuracy: 0.9933 - val_loss: 1.1787 - val_accuracy: 0.7840\n",
            "Epoch 25/100\n",
            "750/750 [==============================] - 0s 623us/sample - loss: 0.0228 - accuracy: 0.9947 - val_loss: 1.0195 - val_accuracy: 0.8120\n",
            "Epoch 26/100\n",
            "750/750 [==============================] - 0s 639us/sample - loss: 0.0276 - accuracy: 0.9947 - val_loss: 1.0055 - val_accuracy: 0.7920\n",
            "Epoch 27/100\n",
            "750/750 [==============================] - 0s 620us/sample - loss: 0.0217 - accuracy: 0.9947 - val_loss: 0.9780 - val_accuracy: 0.8040\n",
            "Epoch 28/100\n",
            "750/750 [==============================] - 0s 628us/sample - loss: 0.0106 - accuracy: 0.9973 - val_loss: 0.9825 - val_accuracy: 0.7960\n",
            "Epoch 29/100\n",
            "750/750 [==============================] - 0s 645us/sample - loss: 0.0070 - accuracy: 1.0000 - val_loss: 1.0323 - val_accuracy: 0.7920\n",
            "Epoch 30/100\n",
            "750/750 [==============================] - 0s 651us/sample - loss: 0.0086 - accuracy: 0.9973 - val_loss: 1.1348 - val_accuracy: 0.7800\n",
            "Epoch 31/100\n",
            "750/750 [==============================] - 0s 641us/sample - loss: 0.0169 - accuracy: 0.9960 - val_loss: 1.2107 - val_accuracy: 0.7880\n",
            "Epoch 32/100\n",
            "750/750 [==============================] - 0s 634us/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 1.2690 - val_accuracy: 0.7840\n",
            "Epoch 33/100\n",
            "750/750 [==============================] - 0s 652us/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 1.3230 - val_accuracy: 0.7720\n",
            "Epoch 34/100\n",
            "750/750 [==============================] - 1s 668us/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 1.3486 - val_accuracy: 0.7720\n",
            "Epoch 35/100\n",
            "750/750 [==============================] - 0s 621us/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.3655 - val_accuracy: 0.7760\n",
            "Epoch 36/100\n",
            "750/750 [==============================] - 0s 643us/sample - loss: 0.0040 - accuracy: 0.9987 - val_loss: 1.4083 - val_accuracy: 0.7840\n",
            "Epoch 37/100\n",
            "750/750 [==============================] - 0s 612us/sample - loss: 0.0090 - accuracy: 0.9987 - val_loss: 1.4107 - val_accuracy: 0.7840\n",
            "Epoch 38/100\n",
            "750/750 [==============================] - 0s 615us/sample - loss: 0.0198 - accuracy: 0.9960 - val_loss: 1.3465 - val_accuracy: 0.7840\n",
            "Epoch 39/100\n",
            "750/750 [==============================] - 1s 676us/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 1.3060 - val_accuracy: 0.7920\n",
            "Epoch 40/100\n",
            "750/750 [==============================] - 0s 638us/sample - loss: 0.0099 - accuracy: 0.9987 - val_loss: 1.2496 - val_accuracy: 0.7880\n",
            "Epoch 41/100\n",
            "750/750 [==============================] - 0s 636us/sample - loss: 0.0095 - accuracy: 0.9987 - val_loss: 1.1401 - val_accuracy: 0.8000\n",
            "Epoch 42/100\n",
            "750/750 [==============================] - 0s 630us/sample - loss: 0.0053 - accuracy: 0.9987 - val_loss: 1.0453 - val_accuracy: 0.7880\n",
            "Epoch 43/100\n",
            "750/750 [==============================] - 0s 646us/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.0791 - val_accuracy: 0.7960\n",
            "Epoch 44/100\n",
            "750/750 [==============================] - 1s 676us/sample - loss: 0.0153 - accuracy: 0.9960 - val_loss: 1.1921 - val_accuracy: 0.8040\n",
            "Epoch 45/100\n",
            "750/750 [==============================] - 0s 664us/sample - loss: 0.0034 - accuracy: 0.9987 - val_loss: 1.3999 - val_accuracy: 0.7840\n",
            "Epoch 46/100\n",
            "750/750 [==============================] - 0s 622us/sample - loss: 0.0053 - accuracy: 0.9987 - val_loss: 1.4302 - val_accuracy: 0.7960\n",
            "Epoch 47/100\n",
            "750/750 [==============================] - 1s 671us/sample - loss: 7.4504e-04 - accuracy: 1.0000 - val_loss: 1.4745 - val_accuracy: 0.7920\n",
            "Epoch 48/100\n",
            "750/750 [==============================] - 0s 629us/sample - loss: 4.2368e-04 - accuracy: 1.0000 - val_loss: 1.5391 - val_accuracy: 0.7880\n",
            "Epoch 49/100\n",
            "750/750 [==============================] - 0s 655us/sample - loss: 0.0169 - accuracy: 0.9973 - val_loss: 1.5749 - val_accuracy: 0.7880\n",
            "Epoch 50/100\n",
            "750/750 [==============================] - 0s 639us/sample - loss: 0.0018 - accuracy: 0.9987 - val_loss: 1.5989 - val_accuracy: 0.7760\n",
            "Epoch 51/100\n",
            "750/750 [==============================] - 1s 682us/sample - loss: 4.4725e-04 - accuracy: 1.0000 - val_loss: 1.5621 - val_accuracy: 0.7920\n",
            "Epoch 52/100\n",
            "750/750 [==============================] - 0s 646us/sample - loss: 0.0017 - accuracy: 0.9987 - val_loss: 1.5716 - val_accuracy: 0.7880\n",
            "Epoch 53/100\n",
            "750/750 [==============================] - 0s 643us/sample - loss: 5.5714e-04 - accuracy: 1.0000 - val_loss: 1.6467 - val_accuracy: 0.7800\n",
            "Epoch 54/100\n",
            "750/750 [==============================] - 0s 653us/sample - loss: 8.7037e-04 - accuracy: 1.0000 - val_loss: 1.6568 - val_accuracy: 0.7720\n",
            "Epoch 55/100\n",
            "750/750 [==============================] - 0s 642us/sample - loss: 0.0110 - accuracy: 0.9973 - val_loss: 1.5231 - val_accuracy: 0.7880\n",
            "Epoch 56/100\n",
            "750/750 [==============================] - 0s 640us/sample - loss: 3.3462e-04 - accuracy: 1.0000 - val_loss: 1.5478 - val_accuracy: 0.7880\n",
            "Epoch 57/100\n",
            "750/750 [==============================] - 0s 634us/sample - loss: 0.0188 - accuracy: 0.9960 - val_loss: 1.5490 - val_accuracy: 0.7840\n",
            "Epoch 58/100\n",
            "750/750 [==============================] - 1s 689us/sample - loss: 5.6478e-04 - accuracy: 1.0000 - val_loss: 1.5230 - val_accuracy: 0.7920\n",
            "Epoch 59/100\n",
            "750/750 [==============================] - 0s 650us/sample - loss: 0.0018 - accuracy: 0.9987 - val_loss: 1.6102 - val_accuracy: 0.7800\n",
            "Epoch 60/100\n",
            "750/750 [==============================] - 0s 644us/sample - loss: 3.6611e-04 - accuracy: 1.0000 - val_loss: 1.7119 - val_accuracy: 0.7760\n",
            "Epoch 61/100\n",
            "750/750 [==============================] - 1s 674us/sample - loss: 4.6152e-04 - accuracy: 1.0000 - val_loss: 1.7557 - val_accuracy: 0.7720\n",
            "Epoch 62/100\n",
            "750/750 [==============================] - 0s 646us/sample - loss: 0.0164 - accuracy: 0.9973 - val_loss: 1.7876 - val_accuracy: 0.7600\n",
            "Epoch 63/100\n",
            "750/750 [==============================] - 0s 644us/sample - loss: 0.0115 - accuracy: 0.9987 - val_loss: 1.8481 - val_accuracy: 0.7440\n",
            "Epoch 64/100\n",
            "750/750 [==============================] - 0s 653us/sample - loss: 0.0168 - accuracy: 0.9960 - val_loss: 1.6651 - val_accuracy: 0.7720\n",
            "Epoch 65/100\n",
            "750/750 [==============================] - 0s 631us/sample - loss: 9.6021e-04 - accuracy: 1.0000 - val_loss: 1.4077 - val_accuracy: 0.7920\n",
            "Epoch 66/100\n",
            "750/750 [==============================] - 0s 622us/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.4467 - val_accuracy: 0.7800\n",
            "Epoch 67/100\n",
            "750/750 [==============================] - 0s 623us/sample - loss: 0.0095 - accuracy: 0.9987 - val_loss: 1.4773 - val_accuracy: 0.7720\n",
            "Epoch 68/100\n",
            "750/750 [==============================] - 0s 613us/sample - loss: 0.0059 - accuracy: 0.9987 - val_loss: 1.3748 - val_accuracy: 0.7800\n",
            "Epoch 69/100\n",
            "750/750 [==============================] - 0s 633us/sample - loss: 0.0096 - accuracy: 0.9973 - val_loss: 1.2463 - val_accuracy: 0.8000\n",
            "Epoch 70/100\n",
            "750/750 [==============================] - 0s 601us/sample - loss: 0.0036 - accuracy: 0.9987 - val_loss: 1.2449 - val_accuracy: 0.7960\n",
            "Epoch 71/100\n",
            "750/750 [==============================] - 0s 622us/sample - loss: 0.0038 - accuracy: 0.9987 - val_loss: 1.3340 - val_accuracy: 0.7840\n",
            "Epoch 72/100\n",
            "750/750 [==============================] - 0s 615us/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 1.4303 - val_accuracy: 0.7800\n",
            "Epoch 73/100\n",
            "750/750 [==============================] - 0s 647us/sample - loss: 0.0086 - accuracy: 0.9987 - val_loss: 1.3830 - val_accuracy: 0.8000\n",
            "Epoch 74/100\n",
            "750/750 [==============================] - 0s 616us/sample - loss: 7.6182e-04 - accuracy: 1.0000 - val_loss: 1.4568 - val_accuracy: 0.7920\n",
            "Epoch 75/100\n",
            "750/750 [==============================] - 0s 654us/sample - loss: 0.0062 - accuracy: 0.9987 - val_loss: 1.5092 - val_accuracy: 0.7840\n",
            "Epoch 76/100\n",
            "750/750 [==============================] - 0s 613us/sample - loss: 0.0022 - accuracy: 0.9987 - val_loss: 1.4883 - val_accuracy: 0.7960\n",
            "Epoch 77/100\n",
            "750/750 [==============================] - 0s 635us/sample - loss: 9.2689e-04 - accuracy: 1.0000 - val_loss: 1.4807 - val_accuracy: 0.8000\n",
            "Epoch 78/100\n",
            "750/750 [==============================] - 0s 617us/sample - loss: 5.2989e-04 - accuracy: 1.0000 - val_loss: 1.4854 - val_accuracy: 0.8000\n",
            "Epoch 79/100\n",
            "750/750 [==============================] - 0s 633us/sample - loss: 0.0111 - accuracy: 0.9987 - val_loss: 1.4870 - val_accuracy: 0.7960\n",
            "Epoch 80/100\n",
            "750/750 [==============================] - 0s 641us/sample - loss: 3.7263e-04 - accuracy: 1.0000 - val_loss: 1.4846 - val_accuracy: 0.7960\n",
            "Epoch 81/100\n",
            "750/750 [==============================] - 0s 613us/sample - loss: 5.9491e-04 - accuracy: 1.0000 - val_loss: 1.4860 - val_accuracy: 0.8000\n",
            "Epoch 82/100\n",
            "750/750 [==============================] - 0s 638us/sample - loss: 3.9038e-04 - accuracy: 1.0000 - val_loss: 1.4911 - val_accuracy: 0.8000\n",
            "Epoch 83/100\n",
            "750/750 [==============================] - 0s 643us/sample - loss: 4.2029e-04 - accuracy: 1.0000 - val_loss: 1.4978 - val_accuracy: 0.8000\n",
            "Epoch 84/100\n",
            "750/750 [==============================] - 0s 639us/sample - loss: 3.6847e-04 - accuracy: 1.0000 - val_loss: 1.5067 - val_accuracy: 0.8000\n",
            "Epoch 85/100\n",
            "750/750 [==============================] - 0s 612us/sample - loss: 3.4774e-04 - accuracy: 1.0000 - val_loss: 1.5170 - val_accuracy: 0.8000\n",
            "Epoch 86/100\n",
            "750/750 [==============================] - 0s 639us/sample - loss: 3.1601e-04 - accuracy: 1.0000 - val_loss: 1.5283 - val_accuracy: 0.8000\n",
            "Epoch 87/100\n",
            "750/750 [==============================] - 0s 601us/sample - loss: 2.7095e-04 - accuracy: 1.0000 - val_loss: 1.5394 - val_accuracy: 0.8000\n",
            "Epoch 88/100\n",
            "750/750 [==============================] - 0s 614us/sample - loss: 2.7529e-04 - accuracy: 1.0000 - val_loss: 1.5506 - val_accuracy: 0.8000\n",
            "Epoch 89/100\n",
            "750/750 [==============================] - 0s 620us/sample - loss: 2.6844e-04 - accuracy: 1.0000 - val_loss: 1.5617 - val_accuracy: 0.8000\n",
            "Epoch 90/100\n",
            "750/750 [==============================] - 0s 602us/sample - loss: 2.5471e-04 - accuracy: 1.0000 - val_loss: 1.5726 - val_accuracy: 0.8000\n",
            "Epoch 91/100\n",
            "750/750 [==============================] - 0s 617us/sample - loss: 7.9914e-04 - accuracy: 1.0000 - val_loss: 1.5877 - val_accuracy: 0.8000\n",
            "Epoch 92/100\n",
            "750/750 [==============================] - 0s 618us/sample - loss: 2.2116e-04 - accuracy: 1.0000 - val_loss: 1.6098 - val_accuracy: 0.7960\n",
            "Epoch 93/100\n",
            "750/750 [==============================] - 0s 616us/sample - loss: 3.0415e-04 - accuracy: 1.0000 - val_loss: 1.6299 - val_accuracy: 0.7960\n",
            "Epoch 94/100\n",
            "750/750 [==============================] - 0s 620us/sample - loss: 2.1705e-04 - accuracy: 1.0000 - val_loss: 1.6468 - val_accuracy: 0.7960\n",
            "Epoch 95/100\n",
            "750/750 [==============================] - 0s 610us/sample - loss: 1.8494e-04 - accuracy: 1.0000 - val_loss: 1.6606 - val_accuracy: 0.7960\n",
            "Epoch 96/100\n",
            "750/750 [==============================] - 0s 628us/sample - loss: 1.6965e-04 - accuracy: 1.0000 - val_loss: 1.6723 - val_accuracy: 0.7960\n",
            "Epoch 97/100\n",
            "750/750 [==============================] - 0s 632us/sample - loss: 1.8125e-04 - accuracy: 1.0000 - val_loss: 1.6829 - val_accuracy: 0.7960\n",
            "Epoch 98/100\n",
            "750/750 [==============================] - 0s 624us/sample - loss: 1.5349e-04 - accuracy: 1.0000 - val_loss: 1.6922 - val_accuracy: 0.7960\n",
            "Epoch 99/100\n",
            "750/750 [==============================] - 0s 659us/sample - loss: 1.5262e-04 - accuracy: 1.0000 - val_loss: 1.7006 - val_accuracy: 0.7960\n",
            "Epoch 100/100\n",
            "750/750 [==============================] - 0s 619us/sample - loss: 2.4910e-04 - accuracy: 1.0000 - val_loss: 1.7102 - val_accuracy: 0.7960\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbTRlAPgcZ1Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "15f657e9-caa9-4093-a26d-88f6536364a4"
      },
      "source": [
        "loss, accuracy = classifier.evaluate(X_test_pad, np.array(y_test), verbose=True)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r250/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 970us/sample - loss: 1.3740 - accuracy: 0.7960\n",
            "Accuracy: 0.796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybGbMXfBgyec",
        "colab_type": "text"
      },
      "source": [
        "## 3.2 **GRU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKt5zAY2g14f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cleaning the text\n",
        "X = movie_df['review']\n",
        "y = movie_df['sentiment']\n",
        "\n",
        "# train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1000)\n",
        "\n",
        "# fit method\n",
        "tokenizer_obj = Tokenizer()\n",
        "tokenizer_obj.fit_on_texts(X_train) \n",
        "\n",
        "# pad sequences\n",
        "max_length = max([len(s.split()) for s in X_train])\n",
        "\n",
        "# define vocabulary size\n",
        "vocab_size = len(tokenizer_obj.word_index) + 1\n",
        "\n",
        "\n",
        "# Transform\n",
        "X_train_tokens =  tokenizer_obj.texts_to_sequences(X_train)\n",
        "X_test_tokens = tokenizer_obj.texts_to_sequences(X_test)\n",
        "\n",
        "# Padding\n",
        "X_train_pad = pad_sequences(X_train_tokens, maxlen=max_length, padding='post')\n",
        "X_test_pad = pad_sequences(X_test_tokens, maxlen=max_length, padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtjqLqnEg8_C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "outputId": "c3398c73-57bb-4804-ffd3-d088ca029e5b"
      },
      "source": [
        "# LSTM\n",
        "embedding_dim =100\n",
        "classifier  = Sequential()\n",
        "classifier.add(Embedding(input_dim=vocab_size, \n",
        "                         output_dim=embedding_dim, \n",
        "                         input_length=max_length))\n",
        "\n",
        "classifier.add(GRU(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "\n",
        "classifier.add(Dense(10, activation = 'relu'))\n",
        "\n",
        "classifier.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "# model summary\n",
        "classifier.summary()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_8 (Embedding)      (None, 30, 100)           157400    \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, 100)               60600     \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 10)                1010      \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 219,021\n",
            "Trainable params: 219,021\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHM5-c2ahAM6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "91d8d6e9-c443-486f-de15-02df81f086c5"
      },
      "source": [
        "# compile the model\n",
        "classifier.compile(loss='binary_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])\n",
        "# Train the model\n",
        "history = classifier.fit(X_train_pad, np.array(y_train),\n",
        "                    epochs=100,\n",
        "                    verbose=1 ,\n",
        "                    validation_data=(X_test_pad, np.array(y_test)),\n",
        "                    batch_size=X_test_pad.shape[0])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 750 samples, validate on 250 samples\n",
            "Epoch 1/100\n",
            "750/750 [==============================] - 2s 3ms/sample - loss: 0.6938 - accuracy: 0.4880 - val_loss: 0.6931 - val_accuracy: 0.5040\n",
            "Epoch 2/100\n",
            "750/750 [==============================] - 1s 743us/sample - loss: 0.6935 - accuracy: 0.4867 - val_loss: 0.6933 - val_accuracy: 0.4960\n",
            "Epoch 3/100\n",
            "750/750 [==============================] - 1s 760us/sample - loss: 0.6933 - accuracy: 0.4787 - val_loss: 0.6934 - val_accuracy: 0.4960\n",
            "Epoch 4/100\n",
            "750/750 [==============================] - 1s 774us/sample - loss: 0.6927 - accuracy: 0.5040 - val_loss: 0.6932 - val_accuracy: 0.4960\n",
            "Epoch 5/100\n",
            "750/750 [==============================] - 1s 720us/sample - loss: 0.6935 - accuracy: 0.4640 - val_loss: 0.6932 - val_accuracy: 0.4920\n",
            "Epoch 6/100\n",
            "750/750 [==============================] - 1s 736us/sample - loss: 0.6923 - accuracy: 0.5307 - val_loss: 0.6931 - val_accuracy: 0.5040\n",
            "Epoch 7/100\n",
            "750/750 [==============================] - 1s 729us/sample - loss: 0.6926 - accuracy: 0.4987 - val_loss: 0.6931 - val_accuracy: 0.5040\n",
            "Epoch 8/100\n",
            "750/750 [==============================] - 1s 738us/sample - loss: 0.6911 - accuracy: 0.5493 - val_loss: 0.6932 - val_accuracy: 0.4960\n",
            "Epoch 9/100\n",
            "750/750 [==============================] - 1s 722us/sample - loss: 0.6939 - accuracy: 0.4800 - val_loss: 0.6931 - val_accuracy: 0.5040\n",
            "Epoch 10/100\n",
            "750/750 [==============================] - 1s 720us/sample - loss: 0.6930 - accuracy: 0.5080 - val_loss: 0.6931 - val_accuracy: 0.5040\n",
            "Epoch 11/100\n",
            "750/750 [==============================] - 1s 730us/sample - loss: 0.6922 - accuracy: 0.5107 - val_loss: 0.6933 - val_accuracy: 0.4960\n",
            "Epoch 12/100\n",
            "750/750 [==============================] - 1s 747us/sample - loss: 0.6885 - accuracy: 0.5613 - val_loss: 0.6932 - val_accuracy: 0.4960\n",
            "Epoch 13/100\n",
            "750/750 [==============================] - 1s 727us/sample - loss: 0.6855 - accuracy: 0.5827 - val_loss: 0.6929 - val_accuracy: 0.5160\n",
            "Epoch 14/100\n",
            "750/750 [==============================] - 1s 731us/sample - loss: 0.6806 - accuracy: 0.5893 - val_loss: 0.6922 - val_accuracy: 0.5320\n",
            "Epoch 15/100\n",
            "750/750 [==============================] - 1s 721us/sample - loss: 0.6502 - accuracy: 0.6707 - val_loss: 0.6910 - val_accuracy: 0.4960\n",
            "Epoch 16/100\n",
            "750/750 [==============================] - 1s 734us/sample - loss: 0.5597 - accuracy: 0.7280 - val_loss: 0.6673 - val_accuracy: 0.5720\n",
            "Epoch 17/100\n",
            "750/750 [==============================] - 1s 753us/sample - loss: 0.5141 - accuracy: 0.7867 - val_loss: 0.6746 - val_accuracy: 0.5240\n",
            "Epoch 18/100\n",
            "750/750 [==============================] - 1s 747us/sample - loss: 0.3841 - accuracy: 0.8547 - val_loss: 0.6559 - val_accuracy: 0.6200\n",
            "Epoch 19/100\n",
            "750/750 [==============================] - 1s 765us/sample - loss: 0.2918 - accuracy: 0.8800 - val_loss: 0.6354 - val_accuracy: 0.6520\n",
            "Epoch 20/100\n",
            "750/750 [==============================] - 1s 746us/sample - loss: 0.1935 - accuracy: 0.9440 - val_loss: 0.5830 - val_accuracy: 0.7760\n",
            "Epoch 21/100\n",
            "750/750 [==============================] - 1s 751us/sample - loss: 0.1451 - accuracy: 0.9560 - val_loss: 0.5650 - val_accuracy: 0.7480\n",
            "Epoch 22/100\n",
            "750/750 [==============================] - 1s 754us/sample - loss: 0.0821 - accuracy: 0.9813 - val_loss: 0.5693 - val_accuracy: 0.7240\n",
            "Epoch 23/100\n",
            "750/750 [==============================] - 1s 754us/sample - loss: 0.0787 - accuracy: 0.9773 - val_loss: 0.5602 - val_accuracy: 0.7520\n",
            "Epoch 24/100\n",
            "750/750 [==============================] - 1s 724us/sample - loss: 0.0335 - accuracy: 0.9907 - val_loss: 0.5394 - val_accuracy: 0.7680\n",
            "Epoch 25/100\n",
            "750/750 [==============================] - 1s 735us/sample - loss: 0.0456 - accuracy: 0.9867 - val_loss: 0.5253 - val_accuracy: 0.7680\n",
            "Epoch 26/100\n",
            "750/750 [==============================] - 1s 727us/sample - loss: 0.0132 - accuracy: 0.9987 - val_loss: 0.5314 - val_accuracy: 0.7400\n",
            "Epoch 27/100\n",
            "750/750 [==============================] - 1s 725us/sample - loss: 0.0270 - accuracy: 0.9933 - val_loss: 0.5105 - val_accuracy: 0.7440\n",
            "Epoch 28/100\n",
            "750/750 [==============================] - 1s 741us/sample - loss: 0.0057 - accuracy: 0.9987 - val_loss: 0.4784 - val_accuracy: 0.7720\n",
            "Epoch 29/100\n",
            "750/750 [==============================] - 1s 725us/sample - loss: 0.0075 - accuracy: 0.9973 - val_loss: 0.4647 - val_accuracy: 0.7840\n",
            "Epoch 30/100\n",
            "750/750 [==============================] - 1s 734us/sample - loss: 0.0119 - accuracy: 0.9987 - val_loss: 0.4679 - val_accuracy: 0.7680\n",
            "Epoch 31/100\n",
            "750/750 [==============================] - 1s 750us/sample - loss: 7.6300e-04 - accuracy: 1.0000 - val_loss: 0.4839 - val_accuracy: 0.7560\n",
            "Epoch 32/100\n",
            "750/750 [==============================] - 1s 749us/sample - loss: 5.5174e-04 - accuracy: 1.0000 - val_loss: 0.5043 - val_accuracy: 0.7560\n",
            "Epoch 33/100\n",
            "750/750 [==============================] - 1s 746us/sample - loss: 9.8188e-04 - accuracy: 1.0000 - val_loss: 0.5226 - val_accuracy: 0.7520\n",
            "Epoch 34/100\n",
            "750/750 [==============================] - 1s 741us/sample - loss: 3.6362e-04 - accuracy: 1.0000 - val_loss: 0.5391 - val_accuracy: 0.7560\n",
            "Epoch 35/100\n",
            "750/750 [==============================] - 1s 736us/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.5449 - val_accuracy: 0.7600\n",
            "Epoch 36/100\n",
            "750/750 [==============================] - 1s 728us/sample - loss: 7.3093e-04 - accuracy: 1.0000 - val_loss: 0.5461 - val_accuracy: 0.7520\n",
            "Epoch 37/100\n",
            "750/750 [==============================] - 1s 731us/sample - loss: 1.5284e-04 - accuracy: 1.0000 - val_loss: 0.5463 - val_accuracy: 0.7560\n",
            "Epoch 38/100\n",
            "750/750 [==============================] - 1s 744us/sample - loss: 0.0027 - accuracy: 0.9987 - val_loss: 0.5229 - val_accuracy: 0.7640\n",
            "Epoch 39/100\n",
            "750/750 [==============================] - 1s 776us/sample - loss: 7.5558e-04 - accuracy: 1.0000 - val_loss: 0.5082 - val_accuracy: 0.7920\n",
            "Epoch 40/100\n",
            "750/750 [==============================] - 1s 719us/sample - loss: 9.0748e-05 - accuracy: 1.0000 - val_loss: 0.5177 - val_accuracy: 0.7920\n",
            "Epoch 41/100\n",
            "750/750 [==============================] - 1s 743us/sample - loss: 4.3822e-04 - accuracy: 1.0000 - val_loss: 0.5378 - val_accuracy: 0.7920\n",
            "Epoch 42/100\n",
            "750/750 [==============================] - 1s 711us/sample - loss: 0.0162 - accuracy: 0.9973 - val_loss: 0.4941 - val_accuracy: 0.7880\n",
            "Epoch 43/100\n",
            "750/750 [==============================] - 1s 759us/sample - loss: 2.3318e-04 - accuracy: 1.0000 - val_loss: 0.4726 - val_accuracy: 0.7760\n",
            "Epoch 44/100\n",
            "750/750 [==============================] - 1s 743us/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.4648 - val_accuracy: 0.7840\n",
            "Epoch 45/100\n",
            "750/750 [==============================] - 1s 737us/sample - loss: 0.0087 - accuracy: 0.9987 - val_loss: 0.4655 - val_accuracy: 0.7760\n",
            "Epoch 46/100\n",
            "750/750 [==============================] - 1s 764us/sample - loss: 5.7273e-04 - accuracy: 1.0000 - val_loss: 0.4852 - val_accuracy: 0.7680\n",
            "Epoch 47/100\n",
            "750/750 [==============================] - 1s 733us/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.5019 - val_accuracy: 0.7560\n",
            "Epoch 48/100\n",
            "750/750 [==============================] - 1s 763us/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.5080 - val_accuracy: 0.7520\n",
            "Epoch 49/100\n",
            "750/750 [==============================] - 1s 705us/sample - loss: 0.0037 - accuracy: 0.9987 - val_loss: 0.4866 - val_accuracy: 0.7880\n",
            "Epoch 50/100\n",
            "750/750 [==============================] - 1s 730us/sample - loss: 0.0058 - accuracy: 0.9987 - val_loss: 0.4903 - val_accuracy: 0.7560\n",
            "Epoch 51/100\n",
            "750/750 [==============================] - 1s 703us/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.5131 - val_accuracy: 0.7240\n",
            "Epoch 52/100\n",
            "750/750 [==============================] - 1s 729us/sample - loss: 0.0058 - accuracy: 0.9987 - val_loss: 0.4708 - val_accuracy: 0.7840\n",
            "Epoch 53/100\n",
            "750/750 [==============================] - 1s 707us/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4678 - val_accuracy: 0.7960\n",
            "Epoch 54/100\n",
            "750/750 [==============================] - 1s 736us/sample - loss: 2.3814e-04 - accuracy: 1.0000 - val_loss: 0.5108 - val_accuracy: 0.7600\n",
            "Epoch 55/100\n",
            "750/750 [==============================] - 1s 710us/sample - loss: 7.0769e-04 - accuracy: 1.0000 - val_loss: 0.5541 - val_accuracy: 0.7280\n",
            "Epoch 56/100\n",
            "750/750 [==============================] - 1s 721us/sample - loss: 0.0063 - accuracy: 0.9987 - val_loss: 0.5304 - val_accuracy: 0.7600\n",
            "Epoch 57/100\n",
            "750/750 [==============================] - 1s 741us/sample - loss: 4.8012e-04 - accuracy: 1.0000 - val_loss: 0.4971 - val_accuracy: 0.7760\n",
            "Epoch 58/100\n",
            "750/750 [==============================] - 1s 743us/sample - loss: 1.5461e-04 - accuracy: 1.0000 - val_loss: 0.4813 - val_accuracy: 0.7960\n",
            "Epoch 59/100\n",
            "750/750 [==============================] - 1s 721us/sample - loss: 2.1372e-04 - accuracy: 1.0000 - val_loss: 0.4768 - val_accuracy: 0.8040\n",
            "Epoch 60/100\n",
            "750/750 [==============================] - 1s 723us/sample - loss: 0.0094 - accuracy: 0.9987 - val_loss: 0.4687 - val_accuracy: 0.8000\n",
            "Epoch 61/100\n",
            "750/750 [==============================] - 1s 780us/sample - loss: 1.4650e-04 - accuracy: 1.0000 - val_loss: 0.4759 - val_accuracy: 0.7840\n",
            "Epoch 62/100\n",
            "750/750 [==============================] - 1s 733us/sample - loss: 1.7161e-04 - accuracy: 1.0000 - val_loss: 0.4959 - val_accuracy: 0.7640\n",
            "Epoch 63/100\n",
            "750/750 [==============================] - 1s 735us/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.5076 - val_accuracy: 0.7560\n",
            "Epoch 64/100\n",
            "750/750 [==============================] - 1s 704us/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.5000 - val_accuracy: 0.7680\n",
            "Epoch 65/100\n",
            "750/750 [==============================] - 1s 739us/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4898 - val_accuracy: 0.7760\n",
            "Epoch 66/100\n",
            "750/750 [==============================] - 1s 702us/sample - loss: 0.0023 - accuracy: 0.9987 - val_loss: 0.4779 - val_accuracy: 0.7920\n",
            "Epoch 67/100\n",
            "750/750 [==============================] - 1s 739us/sample - loss: 8.9486e-04 - accuracy: 1.0000 - val_loss: 0.4691 - val_accuracy: 0.7880\n",
            "Epoch 68/100\n",
            "750/750 [==============================] - 1s 721us/sample - loss: 5.0736e-04 - accuracy: 1.0000 - val_loss: 0.4646 - val_accuracy: 0.7920\n",
            "Epoch 69/100\n",
            "750/750 [==============================] - 1s 735us/sample - loss: 5.9133e-04 - accuracy: 1.0000 - val_loss: 0.4626 - val_accuracy: 0.7920\n",
            "Epoch 70/100\n",
            "750/750 [==============================] - 1s 725us/sample - loss: 2.7850e-04 - accuracy: 1.0000 - val_loss: 0.4624 - val_accuracy: 0.7840\n",
            "Epoch 71/100\n",
            "750/750 [==============================] - 1s 722us/sample - loss: 1.2060e-04 - accuracy: 1.0000 - val_loss: 0.4635 - val_accuracy: 0.7840\n",
            "Epoch 72/100\n",
            "750/750 [==============================] - 1s 735us/sample - loss: 5.3724e-04 - accuracy: 1.0000 - val_loss: 0.4652 - val_accuracy: 0.7920\n",
            "Epoch 73/100\n",
            "750/750 [==============================] - 1s 737us/sample - loss: 2.4772e-04 - accuracy: 1.0000 - val_loss: 0.4755 - val_accuracy: 0.7920\n",
            "Epoch 74/100\n",
            "750/750 [==============================] - 1s 772us/sample - loss: 2.3979e-05 - accuracy: 1.0000 - val_loss: 0.4892 - val_accuracy: 0.7920\n",
            "Epoch 75/100\n",
            "750/750 [==============================] - 1s 766us/sample - loss: 1.0658e-04 - accuracy: 1.0000 - val_loss: 0.5034 - val_accuracy: 0.7960\n",
            "Epoch 76/100\n",
            "750/750 [==============================] - 1s 725us/sample - loss: 1.0858e-04 - accuracy: 1.0000 - val_loss: 0.5163 - val_accuracy: 0.7880\n",
            "Epoch 77/100\n",
            "750/750 [==============================] - 1s 781us/sample - loss: 4.5309e-04 - accuracy: 1.0000 - val_loss: 0.5174 - val_accuracy: 0.7880\n",
            "Epoch 78/100\n",
            "750/750 [==============================] - 1s 739us/sample - loss: 3.9280e-05 - accuracy: 1.0000 - val_loss: 0.5145 - val_accuracy: 0.7880\n",
            "Epoch 79/100\n",
            "750/750 [==============================] - 1s 739us/sample - loss: 8.6498e-05 - accuracy: 1.0000 - val_loss: 0.5136 - val_accuracy: 0.7960\n",
            "Epoch 80/100\n",
            "750/750 [==============================] - 1s 725us/sample - loss: 0.0097 - accuracy: 0.9987 - val_loss: 0.4575 - val_accuracy: 0.7960\n",
            "Epoch 81/100\n",
            "750/750 [==============================] - 1s 745us/sample - loss: 3.0494e-04 - accuracy: 1.0000 - val_loss: 0.4761 - val_accuracy: 0.7920\n",
            "Epoch 82/100\n",
            "750/750 [==============================] - 1s 750us/sample - loss: 0.0036 - accuracy: 0.9987 - val_loss: 0.5031 - val_accuracy: 0.7880\n",
            "Epoch 83/100\n",
            "750/750 [==============================] - 1s 739us/sample - loss: 0.0052 - accuracy: 0.9973 - val_loss: 0.5393 - val_accuracy: 0.7240\n",
            "Epoch 84/100\n",
            "750/750 [==============================] - 1s 741us/sample - loss: 0.0059 - accuracy: 0.9987 - val_loss: 0.5662 - val_accuracy: 0.6680\n",
            "Epoch 85/100\n",
            "750/750 [==============================] - 1s 731us/sample - loss: 0.0035 - accuracy: 0.9987 - val_loss: 0.5526 - val_accuracy: 0.6880\n",
            "Epoch 86/100\n",
            "750/750 [==============================] - 1s 738us/sample - loss: 9.7937e-04 - accuracy: 1.0000 - val_loss: 0.5156 - val_accuracy: 0.7360\n",
            "Epoch 87/100\n",
            "750/750 [==============================] - 1s 719us/sample - loss: 3.9082e-04 - accuracy: 1.0000 - val_loss: 0.4907 - val_accuracy: 0.7600\n",
            "Epoch 88/100\n",
            "750/750 [==============================] - 1s 733us/sample - loss: 0.0021 - accuracy: 0.9987 - val_loss: 0.4528 - val_accuracy: 0.7960\n",
            "Epoch 89/100\n",
            "750/750 [==============================] - 1s 721us/sample - loss: 2.5569e-04 - accuracy: 1.0000 - val_loss: 0.4479 - val_accuracy: 0.8080\n",
            "Epoch 90/100\n",
            "750/750 [==============================] - 1s 719us/sample - loss: 0.0114 - accuracy: 0.9973 - val_loss: 0.4656 - val_accuracy: 0.7800\n",
            "Epoch 91/100\n",
            "750/750 [==============================] - 1s 715us/sample - loss: 1.6109e-04 - accuracy: 1.0000 - val_loss: 0.5282 - val_accuracy: 0.7240\n",
            "Epoch 92/100\n",
            "750/750 [==============================] - 1s 733us/sample - loss: 7.7764e-04 - accuracy: 1.0000 - val_loss: 0.5851 - val_accuracy: 0.6720\n",
            "Epoch 93/100\n",
            "750/750 [==============================] - 1s 722us/sample - loss: 0.0077 - accuracy: 0.9973 - val_loss: 0.4973 - val_accuracy: 0.7760\n",
            "Epoch 94/100\n",
            "750/750 [==============================] - 1s 738us/sample - loss: 4.5170e-04 - accuracy: 1.0000 - val_loss: 0.4697 - val_accuracy: 0.7800\n",
            "Epoch 95/100\n",
            "750/750 [==============================] - 1s 727us/sample - loss: 8.1908e-04 - accuracy: 1.0000 - val_loss: 0.4916 - val_accuracy: 0.7600\n",
            "Epoch 96/100\n",
            "750/750 [==============================] - 1s 759us/sample - loss: 2.9499e-04 - accuracy: 1.0000 - val_loss: 0.5167 - val_accuracy: 0.7360\n",
            "Epoch 97/100\n",
            "750/750 [==============================] - 1s 726us/sample - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.5212 - val_accuracy: 0.7360\n",
            "Epoch 98/100\n",
            "750/750 [==============================] - 1s 735us/sample - loss: 0.0048 - accuracy: 0.9973 - val_loss: 0.4579 - val_accuracy: 0.7880\n",
            "Epoch 99/100\n",
            "750/750 [==============================] - 1s 708us/sample - loss: 2.8872e-04 - accuracy: 1.0000 - val_loss: 0.4903 - val_accuracy: 0.7640\n",
            "Epoch 100/100\n",
            "750/750 [==============================] - 1s 727us/sample - loss: 7.1061e-04 - accuracy: 1.0000 - val_loss: 0.5650 - val_accuracy: 0.7520\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "py08NonLhDjq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "4a2fea0f-33d2-48be-b123-073534d871df"
      },
      "source": [
        "loss, accuracy = classifier.evaluate(X_test_pad, np.array(y_test), verbose=True)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r250/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 995us/sample - loss: 0.5077 - accuracy: 0.7520\n",
            "Accuracy: 0.752\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}